{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDLG7uEpJcKZ",
        "outputId": "c43d6a97-cddb-458b-c5ae-deedd1b9bc6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [100000/20000000] Loss: 0.899510 w: 3.002849 b: 0.101685\n",
            "Epoch: [200000/20000000] Loss: 0.814059 w: 3.002710 b: 0.194132\n",
            "Epoch: [300000/20000000] Loss: 0.736665 w: 3.002578 b: 0.282076\n",
            "Epoch: [400000/20000000] Loss: 0.666710 w: 3.002453 b: 0.365737\n",
            "Epoch: [500000/20000000] Loss: 0.603353 w: 3.002333 b: 0.445324\n",
            "Epoch: [600000/20000000] Loss: 0.546008 w: 3.002220 b: 0.521036\n",
            "Epoch: [700000/20000000] Loss: 0.494124 w: 3.002112 b: 0.593060\n",
            "Epoch: [800000/20000000] Loss: 0.447175 w: 3.002009 b: 0.661577\n",
            "Epoch: [900000/20000000] Loss: 0.404685 w: 3.001911 b: 0.726757\n",
            "Epoch: [1000000/20000000] Loss: 0.366227 w: 3.001818 b: 0.788762\n",
            "Epoch: [1100000/20000000] Loss: 0.331393 w: 3.001729 b: 0.847749\n",
            "Epoch: [1200000/20000000] Loss: 0.299926 w: 3.001645 b: 0.903862\n",
            "Epoch: [1300000/20000000] Loss: 0.271434 w: 3.001565 b: 0.957243\n",
            "Epoch: [1400000/20000000] Loss: 0.245641 w: 3.001489 b: 1.008024\n",
            "Epoch: [1500000/20000000] Loss: 0.222311 w: 3.001416 b: 1.056333\n",
            "Epoch: [1600000/20000000] Loss: 0.201167 w: 3.001347 b: 1.102289\n",
            "Epoch: [1700000/20000000] Loss: 0.182056 w: 3.001282 b: 1.146006\n",
            "Epoch: [1800000/20000000] Loss: 0.164731 w: 3.001219 b: 1.187595\n",
            "Epoch: [1900000/20000000] Loss: 0.149077 w: 3.001160 b: 1.227158\n",
            "Epoch: [2000000/20000000] Loss: 0.134946 w: 3.001103 b: 1.264795\n",
            "Epoch: [2100000/20000000] Loss: 0.122094 w: 3.001050 b: 1.300599\n",
            "Epoch: [2200000/20000000] Loss: 0.110505 w: 3.000998 b: 1.334659\n",
            "Epoch: [2300000/20000000] Loss: 0.099989 w: 3.000950 b: 1.367061\n",
            "Epoch: [2400000/20000000] Loss: 0.090511 w: 3.000904 b: 1.397884\n",
            "Epoch: [2500000/20000000] Loss: 0.081902 w: 3.000860 b: 1.427207\n",
            "Epoch: [2600000/20000000] Loss: 0.074120 w: 3.000818 b: 1.455101\n",
            "Epoch: [2700000/20000000] Loss: 0.067076 w: 3.000778 b: 1.481637\n",
            "Epoch: [2800000/20000000] Loss: 0.060700 w: 3.000740 b: 1.506881\n",
            "Epoch: [2900000/20000000] Loss: 0.054943 w: 3.000704 b: 1.530896\n",
            "Epoch: [3000000/20000000] Loss: 0.049715 w: 3.000670 b: 1.553740\n",
            "Epoch: [3100000/20000000] Loss: 0.044986 w: 3.000637 b: 1.575473\n",
            "Epoch: [3200000/20000000] Loss: 0.040714 w: 3.000606 b: 1.596147\n",
            "Epoch: [3300000/20000000] Loss: 0.036839 w: 3.000577 b: 1.615814\n",
            "Epoch: [3400000/20000000] Loss: 0.033344 w: 3.000548 b: 1.634524\n",
            "Epoch: [3500000/20000000] Loss: 0.030173 w: 3.000522 b: 1.652322\n",
            "Epoch: [3600000/20000000] Loss: 0.027301 w: 3.000496 b: 1.669254\n",
            "Epoch: [3700000/20000000] Loss: 0.024712 w: 3.000472 b: 1.685361\n",
            "Epoch: [3800000/20000000] Loss: 0.022364 w: 3.000449 b: 1.700683\n",
            "Epoch: [3900000/20000000] Loss: 0.020243 w: 3.000427 b: 1.715260\n",
            "Epoch: [4000000/20000000] Loss: 0.018321 w: 3.000406 b: 1.729126\n",
            "Epoch: [4100000/20000000] Loss: 0.016581 w: 3.000387 b: 1.742318\n",
            "Epoch: [4200000/20000000] Loss: 0.015000 w: 3.000368 b: 1.754867\n",
            "Epoch: [4300000/20000000] Loss: 0.013575 w: 3.000350 b: 1.766804\n",
            "Epoch: [4400000/20000000] Loss: 0.012281 w: 3.000333 b: 1.778161\n",
            "Epoch: [4500000/20000000] Loss: 0.011122 w: 3.000317 b: 1.788964\n",
            "Epoch: [4600000/20000000] Loss: 0.010064 w: 3.000301 b: 1.799241\n",
            "Epoch: [4700000/20000000] Loss: 0.009108 w: 3.000287 b: 1.809018\n",
            "Epoch: [4800000/20000000] Loss: 0.008240 w: 3.000273 b: 1.818319\n",
            "Epoch: [4900000/20000000] Loss: 0.007457 w: 3.000259 b: 1.827167\n",
            "Epoch: [5000000/20000000] Loss: 0.006752 w: 3.000247 b: 1.835583\n",
            "Epoch: [5100000/20000000] Loss: 0.006104 w: 3.000235 b: 1.843590\n",
            "Epoch: [5200000/20000000] Loss: 0.005530 w: 3.000223 b: 1.851207\n",
            "Epoch: [5300000/20000000] Loss: 0.005001 w: 3.000212 b: 1.858453\n",
            "Epoch: [5400000/20000000] Loss: 0.004523 w: 3.000202 b: 1.865347\n",
            "Epoch: [5500000/20000000] Loss: 0.004094 w: 3.000192 b: 1.871904\n",
            "Epoch: [5600000/20000000] Loss: 0.003708 w: 3.000183 b: 1.878142\n",
            "Epoch: [5700000/20000000] Loss: 0.003354 w: 3.000174 b: 1.884077\n",
            "Epoch: [5800000/20000000] Loss: 0.003034 w: 3.000166 b: 1.889722\n",
            "Epoch: [5900000/20000000] Loss: 0.002746 w: 3.000158 b: 1.895092\n",
            "Epoch: [6000000/20000000] Loss: 0.002486 w: 3.000150 b: 1.900201\n",
            "Epoch: [6100000/20000000] Loss: 0.002249 w: 3.000143 b: 1.905061\n",
            "Epoch: [6200000/20000000] Loss: 0.002036 w: 3.000136 b: 1.909685\n",
            "Epoch: [6300000/20000000] Loss: 0.001843 w: 3.000129 b: 1.914083\n",
            "Epoch: [6400000/20000000] Loss: 0.001667 w: 3.000123 b: 1.918267\n",
            "Epoch: [6500000/20000000] Loss: 0.001511 w: 3.000117 b: 1.922247\n",
            "Epoch: [6600000/20000000] Loss: 0.001366 w: 3.000111 b: 1.926034\n",
            "Epoch: [6700000/20000000] Loss: 0.001236 w: 3.000106 b: 1.929636\n",
            "Epoch: [6800000/20000000] Loss: 0.001119 w: 3.000100 b: 1.933063\n",
            "Epoch: [6900000/20000000] Loss: 0.001012 w: 3.000096 b: 1.936323\n",
            "Epoch: [7000000/20000000] Loss: 0.000916 w: 3.000091 b: 1.939424\n",
            "Epoch: [7100000/20000000] Loss: 0.000829 w: 3.000086 b: 1.942374\n",
            "Epoch: [7200000/20000000] Loss: 0.000749 w: 3.000082 b: 1.945180\n",
            "Epoch: [7300000/20000000] Loss: 0.000678 w: 3.000078 b: 1.947850\n",
            "Epoch: [7400000/20000000] Loss: 0.000615 w: 3.000074 b: 1.950389\n",
            "Epoch: [7500000/20000000] Loss: 0.000557 w: 3.000071 b: 1.952805\n",
            "Epoch: [7600000/20000000] Loss: 0.000503 w: 3.000067 b: 1.955104\n",
            "Epoch: [7700000/20000000] Loss: 0.000456 w: 3.000064 b: 1.957290\n",
            "Epoch: [7800000/20000000] Loss: 0.000413 w: 3.000061 b: 1.959370\n",
            "Epoch: [7900000/20000000] Loss: 0.000374 w: 3.000058 b: 1.961349\n",
            "Epoch: [8000000/20000000] Loss: 0.000337 w: 3.000055 b: 1.963231\n",
            "Epoch: [8100000/20000000] Loss: 0.000306 w: 3.000052 b: 1.965022\n",
            "Epoch: [8200000/20000000] Loss: 0.000277 w: 3.000050 b: 1.966725\n",
            "Epoch: [8300000/20000000] Loss: 0.000250 w: 3.000048 b: 1.968345\n",
            "Epoch: [8400000/20000000] Loss: 0.000227 w: 3.000045 b: 1.969887\n",
            "Epoch: [8500000/20000000] Loss: 0.000205 w: 3.000043 b: 1.971353\n",
            "Epoch: [8600000/20000000] Loss: 0.000185 w: 3.000041 b: 1.972748\n",
            "Epoch: [8700000/20000000] Loss: 0.000168 w: 3.000039 b: 1.974076\n",
            "Epoch: [8800000/20000000] Loss: 0.000152 w: 3.000037 b: 1.975338\n",
            "Epoch: [8900000/20000000] Loss: 0.000137 w: 3.000035 b: 1.976539\n",
            "Epoch: [9000000/20000000] Loss: 0.000125 w: 3.000033 b: 1.977682\n",
            "Epoch: [9100000/20000000] Loss: 0.000113 w: 3.000032 b: 1.978768\n",
            "Epoch: [9200000/20000000] Loss: 0.000101 w: 3.000030 b: 1.979802\n",
            "Epoch: [9300000/20000000] Loss: 0.000092 w: 3.000029 b: 1.980786\n",
            "Epoch: [9400000/20000000] Loss: 0.000083 w: 3.000027 b: 1.981722\n",
            "Epoch: [9500000/20000000] Loss: 0.000076 w: 3.000026 b: 1.982612\n",
            "Epoch: [9600000/20000000] Loss: 0.000068 w: 3.000025 b: 1.983459\n",
            "Epoch: [9700000/20000000] Loss: 0.000062 w: 3.000023 b: 1.984264\n",
            "Epoch: [9800000/20000000] Loss: 0.000056 w: 3.000022 b: 1.985031\n",
            "Epoch: [9900000/20000000] Loss: 0.000051 w: 3.000021 b: 1.985760\n",
            "Epoch: [10000000/20000000] Loss: 0.000046 w: 3.000020 b: 1.986453\n",
            "Epoch: [10100000/20000000] Loss: 0.000041 w: 3.000019 b: 1.987113\n",
            "Epoch: [10200000/20000000] Loss: 0.000038 w: 3.000018 b: 1.987740\n",
            "Epoch: [10300000/20000000] Loss: 0.000034 w: 3.000018 b: 1.988337\n",
            "Epoch: [10400000/20000000] Loss: 0.000031 w: 3.000017 b: 1.988905\n",
            "Epoch: [10500000/20000000] Loss: 0.000028 w: 3.000016 b: 1.989446\n",
            "Epoch: [10600000/20000000] Loss: 0.000025 w: 3.000015 b: 1.989960\n",
            "Epoch: [10700000/20000000] Loss: 0.000023 w: 3.000014 b: 1.990449\n",
            "Epoch: [10800000/20000000] Loss: 0.000021 w: 3.000014 b: 1.990914\n",
            "Epoch: [10900000/20000000] Loss: 0.000019 w: 3.000013 b: 1.991356\n",
            "Epoch: [11000000/20000000] Loss: 0.000017 w: 3.000012 b: 1.991777\n",
            "Epoch: [11100000/20000000] Loss: 0.000015 w: 3.000012 b: 1.992178\n",
            "Epoch: [11200000/20000000] Loss: 0.000014 w: 3.000011 b: 1.992558\n",
            "Epoch: [11300000/20000000] Loss: 0.000013 w: 3.000011 b: 1.992921\n",
            "Epoch: [11400000/20000000] Loss: 0.000011 w: 3.000010 b: 1.993266\n",
            "Epoch: [11500000/20000000] Loss: 0.000010 w: 3.000010 b: 1.993594\n",
            "Epoch: [11600000/20000000] Loss: 0.000009 w: 3.000009 b: 1.993906\n",
            "Epoch: [11700000/20000000] Loss: 0.000008 w: 3.000009 b: 1.994203\n",
            "Epoch: [11800000/20000000] Loss: 0.000007 w: 3.000008 b: 1.994485\n",
            "Epoch: [11900000/20000000] Loss: 0.000007 w: 3.000008 b: 1.994753\n",
            "Epoch: [12000000/20000000] Loss: 0.000006 w: 3.000008 b: 1.995009\n",
            "Epoch: [12100000/20000000] Loss: 0.000006 w: 3.000007 b: 1.995252\n",
            "Epoch: [12200000/20000000] Loss: 0.000005 w: 3.000007 b: 1.995483\n",
            "Epoch: [12300000/20000000] Loss: 0.000005 w: 3.000007 b: 1.995703\n",
            "Epoch: [12400000/20000000] Loss: 0.000004 w: 3.000006 b: 1.995912\n",
            "Epoch: [12500000/20000000] Loss: 0.000004 w: 3.000006 b: 1.996111\n",
            "Epoch: [12600000/20000000] Loss: 0.000003 w: 3.000006 b: 1.996301\n",
            "Epoch: [12700000/20000000] Loss: 0.000003 w: 3.000005 b: 1.996480\n",
            "Epoch: [12800000/20000000] Loss: 0.000003 w: 3.000005 b: 1.996652\n",
            "Epoch: [12900000/20000000] Loss: 0.000003 w: 3.000005 b: 1.996815\n",
            "Epoch: [13000000/20000000] Loss: 0.000002 w: 3.000004 b: 1.996970\n",
            "Epoch: [13100000/20000000] Loss: 0.000002 w: 3.000004 b: 1.997118\n",
            "Epoch: [13200000/20000000] Loss: 0.000002 w: 3.000004 b: 1.997258\n",
            "Epoch: [13300000/20000000] Loss: 0.000002 w: 3.000004 b: 1.997391\n",
            "Epoch: [13400000/20000000] Loss: 0.000002 w: 3.000004 b: 1.997518\n",
            "Epoch: [13500000/20000000] Loss: 0.000001 w: 3.000004 b: 1.997639\n",
            "Epoch: [13600000/20000000] Loss: 0.000001 w: 3.000003 b: 1.997754\n",
            "Epoch: [13700000/20000000] Loss: 0.000001 w: 3.000003 b: 1.997864\n",
            "Epoch: [13800000/20000000] Loss: 0.000001 w: 3.000003 b: 1.997967\n",
            "Epoch: [13900000/20000000] Loss: 0.000001 w: 3.000003 b: 1.998067\n",
            "Epoch: [14000000/20000000] Loss: 0.000001 w: 3.000003 b: 1.998160\n",
            "Epoch: [14100000/20000000] Loss: 0.000001 w: 3.000003 b: 1.998250\n",
            "Epoch: [14200000/20000000] Loss: 0.000001 w: 3.000002 b: 1.998336\n",
            "Epoch: [14300000/20000000] Loss: 0.000001 w: 3.000002 b: 1.998415\n",
            "Epoch: [14400000/20000000] Loss: 0.000001 w: 3.000002 b: 1.998494\n",
            "Epoch: [14500000/20000000] Loss: 0.000001 w: 3.000002 b: 1.998567\n",
            "Epoch: [14600000/20000000] Loss: 0.000000 w: 3.000002 b: 1.998636\n",
            "Epoch: [14700000/20000000] Loss: 0.000000 w: 3.000002 b: 1.998703\n",
            "Epoch: [14800000/20000000] Loss: 0.000000 w: 3.000002 b: 1.998767\n",
            "Epoch: [14900000/20000000] Loss: 0.000000 w: 3.000002 b: 1.998827\n",
            "Epoch: [15000000/20000000] Loss: 0.000000 w: 3.000002 b: 1.998883\n",
            "Epoch: [15100000/20000000] Loss: 0.000000 w: 3.000001 b: 1.998938\n",
            "Epoch: [15200000/20000000] Loss: 0.000000 w: 3.000001 b: 1.998991\n",
            "Epoch: [15300000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999040\n",
            "Epoch: [15400000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999087\n",
            "Epoch: [15500000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999129\n",
            "Epoch: [15600000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999173\n",
            "Epoch: [15700000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999214\n",
            "Epoch: [15800000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999253\n",
            "Epoch: [15900000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999290\n",
            "Epoch: [16000000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999324\n",
            "Epoch: [16100000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999356\n",
            "Epoch: [16200000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999385\n",
            "Epoch: [16300000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999417\n",
            "Epoch: [16400000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999447\n",
            "Epoch: [16500000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999475\n",
            "Epoch: [16600000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999501\n",
            "Epoch: [16700000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999526\n",
            "Epoch: [16800000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999549\n",
            "Epoch: [16900000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999571\n",
            "Epoch: [17000000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999591\n",
            "Epoch: [17100000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999610\n",
            "Epoch: [17200000/20000000] Loss: 0.000000 w: 3.000001 b: 1.999627\n",
            "Epoch: [17300000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999646\n",
            "Epoch: [17400000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999666\n",
            "Epoch: [17500000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999685\n",
            "Epoch: [17600000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999702\n",
            "Epoch: [17700000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999717\n",
            "Epoch: [17800000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999731\n",
            "Epoch: [17900000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999744\n",
            "Epoch: [18000000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999757\n",
            "Epoch: [18100000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999770\n",
            "Epoch: [18200000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999782\n",
            "Epoch: [18300000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999793\n",
            "Epoch: [18400000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999803\n",
            "Epoch: [18500000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999812\n",
            "Epoch: [18600000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999821\n",
            "Epoch: [18700000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999828\n",
            "Epoch: [18800000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999835\n",
            "Epoch: [18900000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999842\n",
            "Epoch: [19000000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999848\n",
            "Epoch: [19100000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999854\n",
            "Epoch: [19200000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999859\n",
            "Epoch: [19300000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999864\n",
            "Epoch: [19400000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999868\n",
            "Epoch: [19500000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999873\n",
            "Epoch: [19600000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999877\n",
            "Epoch: [19700000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999884\n",
            "Epoch: [19800000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999892\n",
            "Epoch: [19900000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999900\n",
            "Epoch: [20000000/20000000] Loss: 0.000000 w: 3.000000 b: 1.999907\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Here we will manually compute the gradient decent process in ML\n",
        "\n",
        "# Linear Regression\n",
        "# f(x) = w * x + b\n",
        "\n",
        "# For example, we are taking below function\n",
        "# y = f(x) =  (3 * x) + 2\n",
        "\n",
        "# input and target value\n",
        "x = np.array([i for i in range(1,1000)],dtype=np.float32)\n",
        "y = np.array([((3*i)+2) for i in x],dtype=np.float32)\n",
        "\n",
        "# weight initialization\n",
        "w = 0.0\n",
        "b = 0.0\n",
        "\n",
        "# forward pass\n",
        "def forward_pass(x):\n",
        "  return (w*x) + b\n",
        "\n",
        "# calculate loss\n",
        "# we are taking simply mse loss\n",
        "# L = mse_loss(y_p, y_a) = (1/N)*((y_p - y_a)**2) = (1/N)*((w*x + b - y)**2)\n",
        "def mse_loss(y_actual, y_pred):\n",
        "  return ((y_pred - y_actual)**2).mean()\n",
        "\n",
        "# caculating gradient w.r.t w\n",
        "# dL/dw = (1/N)*(2*x*(w*x + b - y)) = (1/N)*(2*x*(y_pred - y_actual))\n",
        "def gradient_w(x, y_actual, y_pred):\n",
        "  return (2*x*(y_pred - y_actual)).mean()\n",
        "\n",
        "# caculating gradient w.r.t b\n",
        "# dL/db = (1/N)*(2*(w*x + b - y)) = (1/N)*(2*(y_pred - y_actual))\n",
        "def gradient_b(y_actual, y_pred):\n",
        "  return (2*(y_pred - y_actual)).mean()\n",
        "\n",
        "# training step\n",
        "learning_rate = 0.000001\n",
        "no_of_iterations = 20000000\n",
        "\n",
        "for epoch in range(1,no_of_iterations+1):\n",
        "\n",
        "  # forward pass\n",
        "  y_pred = forward_pass(x)\n",
        "\n",
        "  # calculate loss\n",
        "  L = mse_loss(y, y_pred)\n",
        "\n",
        "  # calculate gradient\n",
        "  d_L_w = gradient_w(x, y, y_pred)\n",
        "  d_L_b = gradient_b(y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w = w - (learning_rate * d_L_w)\n",
        "  b = b - (learning_rate * d_L_b)\n",
        "\n",
        "  # show results\n",
        "  if(epoch%100000==0):\n",
        "    print(\"Epoch: [%d/%d] Loss: %f w: %f b: %f\"%(epoch, no_of_iterations,L,w,b))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esAAe9aYSUQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUBeLnhbUkeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}